"""
Layer Dropping ê²½ëŸ‰í™”

ê°€ì„¤: ìƒìœ„ ë ˆì´ì–´ë“¤ì€ task-specificí•˜ë¯€ë¡œ ì¼ë¶€ ì œê±°í•´ë„ ê¸°ë³¸ ì„±ëŠ¥ ìœ ì§€ ê°€ëŠ¥
ë°©ë²•: Transformerì˜ ìƒìœ„ Nê°œ ë ˆì´ì–´ë¥¼ ì œê±°

EXAONE-4.0-1.2B: 30 layers â†’ 26~28 layersë¡œ ì¶•ì†Œ
"""

import torch
import copy
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig


def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    return "cpu"


def load_base_model(model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B"):
    """ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ"""
    print(f"Loading base model: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    return model, tokenizer, config


def get_transformer_layers(model):
    """ëª¨ë¸ì—ì„œ transformer layers ëª¨ë“ˆ ì°¾ê¸°"""
    # EXAONE ëª¨ë¸ êµ¬ì¡° íƒìƒ‰
    # ì¼ë°˜ì ì¸ êµ¬ì¡°: model.transformer.layers ë˜ëŠ” model.model.layers
    
    if hasattr(model, 'transformer') and hasattr(model.transformer, 'layers'):
        return model.transformer, 'layers'
    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):
        return model.model, 'layers'
    elif hasattr(model, 'model') and hasattr(model.model, 'decoder') and hasattr(model.model.decoder, 'layers'):
        return model.model.decoder, 'layers'
    else:
        # êµ¬ì¡° íƒìƒ‰
        for name, module in model.named_modules():
            if hasattr(module, 'layers') and len(list(module.layers)) > 0:
                return module, 'layers'
        raise ValueError("Could not find transformer layers in model")


def drop_layers(model, config, layers_to_drop: list[int] = None, 
                num_layers_to_keep: int = None, drop_from: str = "top"):
    """
    ë ˆì´ì–´ ë“œë¡­í•‘
    
    Args:
        model: ì›ë³¸ ëª¨ë¸
        config: ëª¨ë¸ ì„¤ì •
        layers_to_drop: ì œê±°í•  ë ˆì´ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        num_layers_to_keep: ìœ ì§€í•  ë ˆì´ì–´ ìˆ˜ (layers_to_dropê³¼ ë°°íƒ€ì )
        drop_from: 'top' (ìƒìœ„ ë ˆì´ì–´ ì œê±°) ë˜ëŠ” 'bottom' (í•˜ìœ„ ë ˆì´ì–´ ì œê±°)
    
    Returns:
        compressed_model, new_config
    """
    # í˜„ì¬ ë ˆì´ì–´ ìˆ˜ í™•ì¸
    transformer, layers_attr = get_transformer_layers(model)
    original_layers = getattr(transformer, layers_attr)
    num_original_layers = len(original_layers)
    
    print(f"\nğŸ”§ Layer Dropping Configuration:")
    print(f"  Original layers: {num_original_layers}")
    
    # ì œê±°í•  ë ˆì´ì–´ ê²°ì •
    if layers_to_drop is None and num_layers_to_keep is not None:
        num_to_drop = num_original_layers - num_layers_to_keep
        if drop_from == "top":
            # ìƒìœ„ ë ˆì´ì–´ ì œê±° (ë§ˆì§€ë§‰ Nê°œ)
            layers_to_drop = list(range(num_original_layers - num_to_drop, num_original_layers))
        else:
            # í•˜ìœ„ ë ˆì´ì–´ ì œê±° (ì²˜ìŒ Nê°œ)
            layers_to_drop = list(range(num_to_drop))
    
    if layers_to_drop is None:
        layers_to_drop = []
    
    layers_to_keep = [i for i in range(num_original_layers) if i not in layers_to_drop]
    
    print(f"  Layers to drop: {layers_to_drop}")
    print(f"  Layers to keep: {layers_to_keep}")
    print(f"  New layer count: {len(layers_to_keep)}")
    
    # ìƒˆ ë ˆì´ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    new_layers = torch.nn.ModuleList([
        copy.deepcopy(original_layers[i]) for i in layers_to_keep
    ])
    
    # ë ˆì´ì–´ êµì²´
    setattr(transformer, layers_attr, new_layers)
    
    # Config ì—…ë°ì´íŠ¸
    new_config = copy.deepcopy(config)
    new_config.num_hidden_layers = len(layers_to_keep)
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    original_params = sum(p.numel() for p in original_layers.parameters())
    new_params = sum(p.numel() for p in new_layers.parameters())
    reduction = (1 - new_params / original_params) * 100
    
    print(f"\nğŸ“Š Parameter Reduction:")
    print(f"  Original: {original_params:,}")
    print(f"  New: {new_params:,}")
    print(f"  Reduction: {reduction:.1f}%")
    
    return model, new_config


def save_compressed_model(model, tokenizer, config, save_path: str):
    """ì••ì¶•ëœ ëª¨ë¸ ì €ì¥"""
    save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ Saving compressed model to: {save_path}")
    
    # Config ì €ì¥ (ë ˆì´ì–´ ìˆ˜ ì—…ë°ì´íŠ¸ë¨)
    config.save_pretrained(save_path)
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
    model.save_pretrained(save_path, safe_serialization=True)
    
    # í† í¬ë‚˜ì´ì € ì €ì¥
    tokenizer.save_pretrained(save_path)
    
    print("âœ… Model saved successfully!")
    
    # ì €ì¥ëœ íŒŒì¼ ëª©ë¡
    files = list(save_path.glob("*"))
    print(f"\nğŸ“ Saved files:")
    for f in files:
        size_mb = f.stat().st_size / (1024 * 1024)
        print(f"  - {f.name} ({size_mb:.1f} MB)")


def test_generation(model, tokenizer, device: str, prompt: str = "Hello, how are you?"):
    """ì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª Testing generation...")
    
    model = model.to(device)
    model.eval()
    
    messages = [{"role": "user", "content": prompt}]
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_new_tokens=50,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"  Prompt: {prompt}")
    print(f"  Response: {response[:200]}...")
    
    return response


def create_layer_dropped_model(
    model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B",
    num_layers_to_keep: int = 26,
    drop_from: str = "top",
    save_path: str = None,
    test: bool = True
):
    """
    Layer Dropping ëª¨ë¸ ìƒì„± ë©”ì¸ í•¨ìˆ˜
    
    Args:
        model_name: ë² ì´ìŠ¤ ëª¨ë¸
        num_layers_to_keep: ìœ ì§€í•  ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸ 30 -> 26)
        drop_from: 'top' ë˜ëŠ” 'bottom'
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥ ì•ˆí•¨)
        test: ìƒì„± í…ŒìŠ¤íŠ¸ ì—¬ë¶€
    """
    print("=" * 60)
    print("ğŸ”ª Layer Dropping Compression")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, config = load_base_model(model_name)
    
    original_layers = config.num_hidden_layers
    device = get_device()
    
    # ë ˆì´ì–´ ë“œë¡­
    model, new_config = drop_layers(
        model, config,
        num_layers_to_keep=num_layers_to_keep,
        drop_from=drop_from
    )
    
    # í…ŒìŠ¤íŠ¸
    if test:
        test_generation(model, tokenizer, device)
    
    # ì €ì¥
    if save_path:
        save_compressed_model(model, tokenizer, new_config, save_path)
    
    print("\n" + "=" * 60)
    print(f"âœ… Layer Dropping Complete!")
    print(f"   {original_layers} layers â†’ {num_layers_to_keep} layers")
    print("=" * 60)
    
    return model, tokenizer, new_config


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Layer Dropping Compression")
    parser.add_argument("--model", type=str, default="LGAI-EXAONE/EXAONE-4.0-1.2B",
                        help="Base model name")
    parser.add_argument("--keep-layers", type=int, default=26,
                        help="Number of layers to keep (default: 26, original: 30)")
    parser.add_argument("--drop-from", type=str, default="top", choices=["top", "bottom"],
                        help="Drop layers from 'top' or 'bottom'")
    parser.add_argument("--save-path", type=str, default=None,
                        help="Path to save compressed model")
    parser.add_argument("--no-test", action="store_true",
                        help="Skip generation test")
    
    args = parser.parse_args()
    
    create_layer_dropped_model(
        model_name=args.model,
        num_layers_to_keep=args.keep_layers,
        drop_from=args.drop_from,
        save_path=args.save_path,
        test=not args.no_test
    )
