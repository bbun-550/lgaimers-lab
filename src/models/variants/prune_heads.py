"""
Attention Head Pruning ê²½ëŸ‰í™”

ê°€ì„¤: ì¼ë¶€ attention headëŠ” redundantí•˜ë¯€ë¡œ ì œê±°í•´ë„ ì„±ëŠ¥ ìœ ì§€ ê°€ëŠ¥
ë°©ë²•: ê° ë ˆì´ì–´ì—ì„œ ì¤‘ìš”ë„ê°€ ë‚®ì€ attention head ì œê±°

EXAONE-4.0-1.2B êµ¬ì¡°:
- num_attention_heads: 32
- num_key_value_heads: 8 (GQA - Grouped Query Attention)
- head_dim: 64

ì£¼ì˜: GQA êµ¬ì¡°ì—ì„œëŠ” Q headsì™€ KV heads ë¹„ìœ¨ì„ ìœ ì§€í•´ì•¼ í•¨
"""

import torch
import copy
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import torch.nn as nn


def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    return "cpu"


def load_base_model(model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B"):
    """ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ"""
    print(f"Loading base model: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    return model, tokenizer, config


def get_attention_info(config):
    """ëª¨ë¸ attention êµ¬ì¡° ì •ë³´ ì¶”ì¶œ"""
    info = {
        "num_attention_heads": getattr(config, "num_attention_heads", 32),
        "num_key_value_heads": getattr(config, "num_key_value_heads", 8),
        "head_dim": getattr(config, "head_dim", 64),
        "hidden_size": getattr(config, "hidden_size", 2048),
        "num_layers": getattr(config, "num_hidden_layers", 30),
    }
    
    # GQA ratio ê³„ì‚°
    info["gqa_ratio"] = info["num_attention_heads"] // info["num_key_value_heads"]
    
    print(f"\nğŸ“Š Attention Configuration:")
    for key, value in info.items():
        print(f"  {key}: {value}")
    
    return info


def get_transformer_layers(model):
    """ëª¨ë¸ì—ì„œ transformer layers ëª¨ë“ˆ ì°¾ê¸°"""
    if hasattr(model, 'transformer') and hasattr(model.transformer, 'layers'):
        return model.transformer, 'layers'
    elif hasattr(model, 'model') and hasattr(model.model, 'layers'):
        return model.model, 'layers'
    elif hasattr(model, 'model') and hasattr(model.model, 'decoder') and hasattr(model.model.decoder, 'layers'):
        return model.model.decoder, 'layers'
    else:
        for name, module in model.named_modules():
            if hasattr(module, 'layers') and len(list(module.layers)) > 0:
                return module, 'layers'
        raise ValueError("Could not find transformer layers in model")


def prune_linear_layer(layer: nn.Linear, keep_indices: list, dim: int = 0):
    """Linear ë ˆì´ì–´ì—ì„œ íŠ¹ì • ì¸ë±ìŠ¤ë§Œ ìœ ì§€"""
    weight = layer.weight.data
    bias = layer.bias.data if layer.bias is not None else None
    
    if dim == 0:  # output features
        new_weight = weight[keep_indices, :].clone()
        new_bias = bias[keep_indices].clone() if bias is not None else None
        new_out_features = len(keep_indices)
        new_in_features = weight.shape[1]
    else:  # input features (dim == 1)
        new_weight = weight[:, keep_indices].clone()
        new_bias = bias.clone() if bias is not None else None
        new_out_features = weight.shape[0]
        new_in_features = len(keep_indices)
    
    new_layer = nn.Linear(new_in_features, new_out_features, bias=bias is not None)
    new_layer.weight.data = new_weight
    if new_bias is not None:
        new_layer.bias.data = new_bias
    
    return new_layer


def prune_attention_heads(model, config, num_heads_to_keep: int, prune_kv: bool = True):
    """
    Attention Head Pruning
    
    Args:
        model: ì›ë³¸ ëª¨ë¸
        config: ëª¨ë¸ ì„¤ì •
        num_heads_to_keep: ìœ ì§€í•  Q head ìˆ˜
        prune_kv: KV headsë„ ë¹„ìœ¨ì— ë§ê²Œ pruningí• ì§€ ì—¬ë¶€
    
    Returns:
        pruned_model, new_config
    """
    attn_info = get_attention_info(config)
    
    original_q_heads = attn_info["num_attention_heads"]
    original_kv_heads = attn_info["num_key_value_heads"]
    head_dim = attn_info["head_dim"]
    gqa_ratio = attn_info["gqa_ratio"]
    
    # ìœ ì§€í•  head ìˆ˜ ê³„ì‚°
    new_q_heads = num_heads_to_keep
    if prune_kv:
        # GQA ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ KV headsë„ ê°ì†Œ
        if new_q_heads % gqa_ratio != 0:
            raise ValueError(
                f"num_heads_to_keep ({new_q_heads}) must be divisible by gqa_ratio ({gqa_ratio})"
            )
        new_kv_heads = max(1, new_q_heads // gqa_ratio)
    else:
        new_kv_heads = original_kv_heads
    
    print(f"\nğŸ”§ Head Pruning Configuration:")
    print(f"  Q Heads: {original_q_heads} â†’ {new_q_heads}")
    print(f"  KV Heads: {original_kv_heads} â†’ {new_kv_heads}")
    print(f"  Head dim: {head_dim} (unchanged)")
    
    # ìœ ì§€í•  head indices (ì•ì—ì„œë¶€í„°)
    q_head_indices = list(range(new_q_heads))
    kv_head_indices = list(range(new_kv_heads))
    
    # Q, K, V projection í¬ê¸° ê³„ì‚°
    q_features = [h * head_dim + i for h in q_head_indices for i in range(head_dim)]
    kv_features = [h * head_dim + i for h in kv_head_indices for i in range(head_dim)]
    
    new_q_dim = new_q_heads * head_dim
    new_kv_dim = new_kv_heads * head_dim
    
    print(f"  Q projection: {original_q_heads * head_dim} â†’ {new_q_dim}")
    print(f"  KV projection: {original_kv_heads * head_dim} â†’ {new_kv_dim}")
    
    # Transformer layers ê°€ì ¸ì˜¤ê¸°
    transformer, layers_attr = get_transformer_layers(model)
    layers = getattr(transformer, layers_attr)
    
    # ê° ë ˆì´ì–´ì˜ attention ìˆ˜ì •
    for layer_idx, layer in enumerate(layers):
        # Attention ëª¨ë“ˆ ì°¾ê¸°
        attn = None
        for name in ['self_attn', 'attention', 'attn']:
            if hasattr(layer, name):
                attn = getattr(layer, name)
                break
        
        if attn is None:
            print(f"  âš ï¸ Layer {layer_idx}: Could not find attention module")
            continue
        
        # Q, K, V projection ì°¾ê¸°
        q_proj = getattr(attn, 'q_proj', None)
        k_proj = getattr(attn, 'k_proj', None)
        v_proj = getattr(attn, 'v_proj', None)
        o_proj = getattr(attn, 'o_proj', None)
        
        if q_proj is None:
            print(f"  âš ï¸ Layer {layer_idx}: Could not find q_proj")
            continue
        
        # Q projection pruning
        attn.q_proj = prune_linear_layer(q_proj, q_features, dim=0)
        
        # K, V projection pruning (if applicable)
        if prune_kv and k_proj is not None:
            attn.k_proj = prune_linear_layer(k_proj, kv_features, dim=0)
        if prune_kv and v_proj is not None:
            attn.v_proj = prune_linear_layer(v_proj, kv_features, dim=0)
        
        # Output projection pruning (input dim matches Q heads)
        if o_proj is not None:
            attn.o_proj = prune_linear_layer(o_proj, q_features, dim=1)
        
        # Attention ëª¨ë“ˆì˜ head ìˆ˜ ì—…ë°ì´íŠ¸
        if hasattr(attn, 'num_heads'):
            attn.num_heads = new_q_heads
        if hasattr(attn, 'num_key_value_heads'):
            attn.num_key_value_heads = new_kv_heads
    
    # Config ì—…ë°ì´íŠ¸
    new_config = copy.deepcopy(config)
    new_config.num_attention_heads = new_q_heads
    new_config.num_key_value_heads = new_kv_heads
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"\nğŸ“Š After Pruning:")
    print(f"  Total parameters: {total_params:,}")
    
    return model, new_config


def prune_kv_heads(model, config, num_kv_heads_to_keep: int):
    """
    KV Headë§Œ Pruning (Q heads ìœ ì§€)
    
    Args:
        model: ì›ë³¸ ëª¨ë¸
        config: ëª¨ë¸ ì„¤ì •
        num_kv_heads_to_keep: ìœ ì§€í•  KV head ìˆ˜
    
    Returns:
        pruned_model, new_config
    """
    attn_info = get_attention_info(config)
    
    original_q_heads = attn_info["num_attention_heads"]
    original_kv_heads = attn_info["num_key_value_heads"]
    head_dim = attn_info["head_dim"]
    
    new_kv_heads = num_kv_heads_to_keep
    new_gqa_ratio = original_q_heads // new_kv_heads
    
    print(f"\nğŸ”§ KV-Only Pruning Configuration:")
    print(f"  Q Heads: {original_q_heads} (unchanged)")
    print(f"  KV Heads: {original_kv_heads} â†’ {new_kv_heads}")
    print(f"  New GQA Ratio: {new_gqa_ratio}")
    print(f"  Head dim: {head_dim} (unchanged)")
    
    # KV head indices
    kv_head_indices = list(range(new_kv_heads))
    kv_features = [h * head_dim + i for h in kv_head_indices for i in range(head_dim)]
    
    new_kv_dim = new_kv_heads * head_dim
    print(f"  KV projection: {original_kv_heads * head_dim} â†’ {new_kv_dim}")
    
    # Transformer layers ê°€ì ¸ì˜¤ê¸°
    transformer, layers_attr = get_transformer_layers(model)
    layers = getattr(transformer, layers_attr)
    
    for layer_idx, layer in enumerate(layers):
        attn = None
        for name in ['self_attn', 'attention', 'attn']:
            if hasattr(layer, name):
                attn = getattr(layer, name)
                break
        
        if attn is None:
            continue
        
        k_proj = getattr(attn, 'k_proj', None)
        v_proj = getattr(attn, 'v_proj', None)
        
        # K, V projectionë§Œ pruning
        if k_proj is not None:
            attn.k_proj = prune_linear_layer(k_proj, kv_features, dim=0)
        if v_proj is not None:
            attn.v_proj = prune_linear_layer(v_proj, kv_features, dim=0)
        
        # Attention ëª¨ë“ˆì˜ KV head ìˆ˜ ì—…ë°ì´íŠ¸
        if hasattr(attn, 'num_key_value_heads'):
            attn.num_key_value_heads = new_kv_heads
    
    # Config ì—…ë°ì´íŠ¸
    new_config = copy.deepcopy(config)
    new_config.num_key_value_heads = new_kv_heads
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ“Š After KV Pruning:")
    print(f"  Total parameters: {total_params:,}")
    
    return model, new_config


def save_compressed_model(model, tokenizer, config, save_path: str):
    """ì••ì¶•ëœ ëª¨ë¸ ì €ì¥"""
    save_path = Path(save_path)
    save_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ Saving compressed model to: {save_path}")
    
    # ëª¨ë¸/Config ì €ì¥ (ëª¨ë¸ì˜ configë¥¼ ìµœì‹ ìœ¼ë¡œ ë§ì¶˜ ë’¤ ì €ì¥)
    model.config = config
    config.save_pretrained(save_path)
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥
    model.save_pretrained(save_path, safe_serialization=True)
    
    # í† í¬ë‚˜ì´ì € ì €ì¥
    tokenizer.save_pretrained(save_path)
    
    print("âœ… Model saved successfully!")
    
    # ì €ì¥ëœ íŒŒì¼ ëª©ë¡
    files = list(save_path.glob("*"))
    print(f"\nğŸ“ Saved files:")
    for f in files:
        size_mb = f.stat().st_size / (1024 * 1024)
        print(f"  - {f.name} ({size_mb:.1f} MB)")


def test_generation(model, tokenizer, device: str, prompt: str = "Hello, how are you?"):
    """ì••ì¶•ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª Testing generation...")
    
    model = model.to(device)
    model.eval()
    
    messages = [{"role": "user", "content": prompt}]
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(device)
    
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_new_tokens=50,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print(f"  Prompt: {prompt}")
    print(f"  Response: {response[:200]}...")
    
    return response


def create_head_pruned_model(
    model_name: str = "LGAI-EXAONE/EXAONE-4.0-1.2B",
    num_heads_to_keep: int = 24,
    prune_kv: bool = True,
    save_path: str = None,
    test: bool = True
):
    """
    Head Pruning ëª¨ë¸ ìƒì„± ë©”ì¸ í•¨ìˆ˜
    
    Args:
        model_name: ë² ì´ìŠ¤ ëª¨ë¸
        num_heads_to_keep: ìœ ì§€í•  Q attention head ìˆ˜ (ê¸°ë³¸ 32 -> 24)
        prune_kv: KV headsë„ ë¹„ìœ¨ì— ë§ê²Œ pruning
        save_path: ì €ì¥ ê²½ë¡œ
        test: ìƒì„± í…ŒìŠ¤íŠ¸ ì—¬ë¶€
    """
    print("=" * 60)
    print("âœ‚ï¸ Attention Head Pruning Compression")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, config = load_base_model(model_name)
    
    original_heads = config.num_attention_heads
    device = get_device()
    
    # Head Pruning
    model, new_config = prune_attention_heads(
        model, config,
        num_heads_to_keep=num_heads_to_keep,
        prune_kv=prune_kv
    )
    
    # í…ŒìŠ¤íŠ¸
    if test:
        test_generation(model, tokenizer, device)
    
    # ì €ì¥
    if save_path:
        save_compressed_model(model, tokenizer, new_config, save_path)
    
    print("\n" + "=" * 60)
    print(f"âœ… Head Pruning Complete!")
    print(f"   {original_heads} heads â†’ {num_heads_to_keep} heads")
    print("=" * 60)
    
    return model, tokenizer, new_config


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Attention Head Pruning Compression")
    parser.add_argument("--model", type=str, default="LGAI-EXAONE/EXAONE-4.0-1.2B",
                        help="Base model name")
    parser.add_argument("--keep-heads", type=int, default=24,
                        help="Number of Q heads to keep (default: 24, original: 32)")
    parser.add_argument("--no-prune-kv", action="store_true",
                        help="Don't prune KV heads (keep original)")
    parser.add_argument("--save-path", type=str, default=None,
                        help="Path to save compressed model")
    parser.add_argument("--no-test", action="store_true",
                        help="Skip generation test")
    
    args = parser.parse_args()
    
    create_head_pruned_model(
        model_name=args.model,
        num_heads_to_keep=args.keep_heads,
        prune_kv=not args.no_prune_kv,
        save_path=args.save_path,
        test=not args.no_test
    )
