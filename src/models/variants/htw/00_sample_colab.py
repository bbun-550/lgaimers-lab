"""
GPTQ ì–‘ìí™” ìµœì í™” ë²„ì „ - Google Colab ì „ìš©
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ìš©ë²•:
1. Colabì—ì„œ GPU ëŸ°íƒ€ì„ ì„ íƒ (T4 ì´ìƒ ê¶Œì¥)
2. ì•„ë˜ ì½”ë“œë¥¼ ì…€ì— ë³µì‚¬í•˜ì—¬ ì‹¤í–‰

ì£¼ì˜: Colab ë¬´ë£Œ ë²„ì „ì€ ë©”ëª¨ë¦¬ ì œí•œì´ ìˆì–´ OOM ë°œìƒ ê°€ëŠ¥
     â†’ num_calibration_samplesë‚˜ max_seq_length ì¤„ì´ê¸°
"""

# =========================================================
# 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Colabì—ì„œ ë¨¼ì € ì‹¤í–‰!)
# =========================================================
# !pip install -q llmcompressor transformers datasets accelerate

import os
import torch
import shutil
from pathlib import Path

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

# =========================================================
# 1. ê²½ë¡œ ì„¤ì • (Colabìš©)
# =========================================================
# HuggingFaceì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (ì•½ 2.5GB)
MODEL_ID = "LGAI-EXAONE/EXAONE-4.0-1.2B"

# ì¶œë ¥ í´ë” (Colab í™˜ê²½)
OUT_DIR = "/content/model"

# =========================================================
# 2. ë°ì´í„°ì…‹ ì„¤ì •
# =========================================================
DATASET_ID = "LGAI-EXAONE/MANTA-1M"
DATASET_SPLIT = "train"

# â­ Colab ë©”ëª¨ë¦¬ ê³ ë ¤ ì„¤ì •
# - Colab Pro: 512 ìƒ˜í”Œ, 1024 ê¸¸ì´ ê°€ëŠ¥
# - Colab Free: 256 ìƒ˜í”Œ, 512 ê¸¸ì´ ê¶Œì¥
NUM_CALIBRATION_SAMPLES = 256  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 128ë¡œ ì¤„ì´ê¸°
MAX_SEQUENCE_LENGTH = 512      # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 256ìœ¼ë¡œ ì¤„ì´ê¸°

# =========================================================
# 3. ì–‘ìí™” ì„¤ì • (ìµœì í™”)
# =========================================================
SCHEME = "W4A16"
TARGETS = ["Linear"]
IGNORE = ["embed_tokens", "lm_head"]
ACTORDER = "static"           # í™œì„±í™” ì •ë ¬ ('static', 'group', 'weight', 'dynamic')
DAMPENING_FRAC = 0.01

# =========================================================
# 4. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
# =========================================================
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"[INFO] GPU: {torch.cuda.get_device_name(0)}")
    print(f"[INFO] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("[WARNING] GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼).")

# =========================================================
# 5. ëª¨ë¸ ë¡œë“œ
# =========================================================
print("\n" + "=" * 60)
print(f"[INFO] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... ({MODEL_ID})")
print("       (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì•½ 2.5GB ë‹¤ìš´ë¡œë“œ)")
print("=" * 60)

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    trust_remote_code=True,
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

print(f"[INFO] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
print(f"       íŒŒë¼ë¯¸í„°: {model.num_parameters() / 1e9:.2f}B")

# =========================================================
# 6. ë°ì´í„°ì…‹ ë¡œë“œ & ì „ì²˜ë¦¬
# =========================================================
print("\n" + "=" * 60)
print(f"[INFO] ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
print(f"       ë°ì´í„°ì…‹: {DATASET_ID}")
print(f"       ìƒ˜í”Œ ìˆ˜: {NUM_CALIBRATION_SAMPLES}")
print("=" * 60)

ds = load_dataset(
    DATASET_ID,
    split=f"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]",
)

def preprocess(example):
    return {
        "text": tokenizer.apply_chat_template(
            example["conversations"],
            add_generation_prompt=True,
            tokenize=False
        )
    }

ds = ds.map(preprocess)
print(f"[INFO] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ({len(ds)}ê°œ ìƒ˜í”Œ)")

# =========================================================
# 7. GPTQ ì–‘ìí™”
# =========================================================
print("\n" + "=" * 60)
print("[INFO] GPTQ ì–‘ìí™” ì‹œì‘ (ì•½ 5~10ë¶„ ì†Œìš”)")
print(f"       Scheme: {SCHEME}")
print(f"       ActOrder: {ACTORDER}")
print(f"       Max Seq Length: {MAX_SEQUENCE_LENGTH}")
print("=" * 60)

recipe = [
    GPTQModifier(
        scheme=SCHEME,
        targets=TARGETS,
        ignore=IGNORE,
        actorder=ACTORDER,
        dampening_frac=DAMPENING_FRAC,
    )
]

oneshot(
    model=model,
    dataset=ds,
    recipe=recipe,
    max_seq_length=MAX_SEQUENCE_LENGTH,
    num_calibration_samples=NUM_CALIBRATION_SAMPLES,
)

print("[INFO] GPTQ ì–‘ìí™” ì™„ë£Œ!")

# =========================================================
# 8. ëª¨ë¸ ì €ì¥
# =========================================================
print(f"\n[INFO] ëª¨ë¸ ì €ì¥ ì¤‘... â†’ {OUT_DIR}")

os.makedirs(OUT_DIR, exist_ok=True)
model.save_pretrained(OUT_DIR, save_compressed=True)
tokenizer.save_pretrained(OUT_DIR)

# ì €ì¥ í™•ì¸
print("[INFO] ì €ì¥ëœ íŒŒì¼:")
for f in os.listdir(OUT_DIR):
    size = os.path.getsize(os.path.join(OUT_DIR, f)) / (1024 * 1024)
    print(f"       - {f} ({size:.1f} MB)")

# =========================================================
# 9. ZIP ìƒì„± & ë‹¤ìš´ë¡œë“œ
# =========================================================
zip_name = "optimized_submit"
print(f"\n[INFO] {zip_name}.zip ìƒì„± ì¤‘...")

shutil.make_archive(
    base_name=f"/content/{zip_name}",
    format="zip",
    root_dir="/content",
    base_dir="model",
)

zip_path = f"/content/{zip_name}.zip"
zip_size = os.path.getsize(zip_path) / (1024 * 1024)
print(f"[INFO] ìƒì„± ì™„ë£Œ: {zip_path} ({zip_size:.1f} MB)")

# Colabì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
try:
    from google.colab import files
    print("\n[INFO] íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    files.download(zip_path)
except ImportError:
    print(f"\n[INFO] Colab í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: {zip_path}")

# =========================================================
# ì™„ë£Œ!
# =========================================================
print("\n" + "=" * 60)
print("âœ… ì–‘ìí™” ì™„ë£Œ!")
print("=" * 60)
print(f"""
ğŸ“Š ì„¤ì • ìš”ì•½:
   â€¢ Model: {MODEL_ID}
   â€¢ Scheme: {SCHEME}
   â€¢ ActOrder: {ACTORDER}
   â€¢ Samples: {NUM_CALIBRATION_SAMPLES}
   â€¢ Max Length: {MAX_SEQUENCE_LENGTH}

ğŸ“ ì¶œë ¥:
   â€¢ ëª¨ë¸: {OUT_DIR}/
   â€¢ ZIP: {zip_path}

ğŸš€ ë‹¤ìš´ë¡œë“œëœ ZIP íŒŒì¼ì„ ëŒ€íšŒì— ì œì¶œí•˜ì„¸ìš”!
""")
