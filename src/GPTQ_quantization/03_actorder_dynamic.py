"""
GPTQ ì–‘ìí™” ìµœì í™” ë²„ì „ - Kaggle Notebook ì „ìš© (T4 x 2)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ì‚¬ìš©ë²•:
1. Kaggle Notebook ìƒì„± (Accelerator: GPU T4 x 2 ì„ íƒ)
2. Internet Access: ON ì„¤ì •
3. ì•„ë˜ ì½”ë“œë¥¼ ì…€ì— ë³µì‚¬í•˜ì—¬ ì‹¤í–‰

íŠ¹ì§•:
- 00_sample_colab3.pyë¥¼ ê¸°ë°˜ìœ¼ë¡œ Kaggle ê²½ë¡œ ë° í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •ë¨
- ë©”ëª¨ë¦¬(32GB) í™œìš©ì„ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ì™€ ì‹œí€€ìŠ¤ ê¸¸ì´ ìƒí–¥
"""

# =========================================================
# 0. íŒ¨í‚¤ì§€ ì„¤ì¹˜ (Kaggleì—ì„œ ë¨¼ì € ì‹¤í–‰!)
# =========================================================
# !pip install -q llmcompressor transformers datasets accelerate dagshub mlflow

import os
os.environ['DAGSHUB_USER_TOKEN'] = '1ee266cf0159abb2c8ad8ae564274c6918599acd'
import torch
import shutil
import time
from pathlib import Path
import mlflow
import dagshub

from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

from llmcompressor import oneshot
from llmcompressor.modifiers.quantization import GPTQModifier

# =========================================================
# DagsHub + MLflow ì—°ê²°
# =========================================================
dagshub.init(repo_owner='sthun0211', repo_name='LGaimers', mlflow=True)
mlflow.set_experiment("htw-actorder-dynamic")

# =========================================================
# 1. ê²½ë¡œ ì„¤ì • (Kaggleìš©)
# =========================================================
# HuggingFaceì—ì„œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
MODEL_ID = "LGAI-EXAONE/EXAONE-4.0-1.2B"

# ì¶œë ¥ í´ë” (Kaggle í™˜ê²½: /kaggle/working)
OUT_DIR = "/kaggle/working/model"

# =========================================================
# 2. ë°ì´í„°ì…‹ ì„¤ì •
# =========================================================
DATASET_ID = "LGAI-EXAONE/MANTA-1M"
DATASET_SPLIT = "train"

# â­ Kaggle T4 x 2 (32GB VRAM) ìµœì í™” ì„¤ì •
# Colab ë²„ì „ì„ ê¸°ë°˜ìœ¼ë¡œ, ë„‰ë„‰í•œ VRAMì„ í™œìš©í•´ ì •í™•ë„ë¥¼ ë†’ì„
NUM_CALIBRATION_SAMPLES = 256   # (Colab 512 -> Kaggle 1024)
MAX_SEQUENCE_LENGTH = 512      # (Colab 1024 -> Kaggle 2048)

# =========================================================
# 3. ì–‘ìí™” ì„¤ì • (ìµœì í™”)
# =========================================================
SCHEME = "W4A16"
TARGETS = ["Linear"]
IGNORE = ["embed_tokens", "lm_head"]
ACTORDER = "dynamic"           # ê³¼ì í•© ë°©ì§€, ì¼ë°˜í™” ì„±ëŠ¥ ìš°ìˆ˜
DAMPENING_FRAC = 0.01

# =========================================================
# 4. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
# =========================================================
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"[INFO] GPU Count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"[INFO] GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"       VRAM: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
else:
    print("[WARNING] GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦¼).")

# =========================================================
# MLflow ì‹¤í—˜ ê¸°ë¡ ì‹œì‘
# =========================================================
with mlflow.start_run(run_name="actorder-dynamic"):

    # ì„¤ì •ê°’(params) ê¸°ë¡
    mlflow.log_params({
        "model_id": MODEL_ID,
        "dataset_id": DATASET_ID,
        "calibration_samples": NUM_CALIBRATION_SAMPLES,
        "max_seq_length": MAX_SEQUENCE_LENGTH,
        "quantization_scheme": SCHEME,
        "targets": str(TARGETS),
        "ignore": str(IGNORE),
        "actorder": ACTORDER,
        "dampening_frac": DAMPENING_FRAC,
    })

# =========================================================
# 5. ëª¨ë¸ ë¡œë“œ
# =========================================================
    print("\n" + "=" * 60)
    print(f"[INFO] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... ({MODEL_ID})")
    print("=" * 60)

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
    )

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,  # T4ì—ì„œëŠ” bfloat16 ë¯¸ì§€ì›ì´ë¯€ë¡œ float16 ì‚¬ìš©
        device_map="auto",          # Multi-GPU ìë™ ë¶„ì‚°
        trust_remote_code=True,
    )

    print(f"[INFO] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    print(f"       íŒŒë¼ë¯¸í„°: {model.num_parameters() / 1e9:.2f}B")

# =========================================================
# 6. ë°ì´í„°ì…‹ ë¡œë“œ & ì „ì²˜ë¦¬
# =========================================================
    print("\n" + "=" * 60)
    print(f"[INFO] ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
    print(f"       ë°ì´í„°ì…‹: {DATASET_ID}")
    print(f"       ìƒ˜í”Œ ìˆ˜: {NUM_CALIBRATION_SAMPLES}")
    print("=" * 60)

    ds = load_dataset(
        DATASET_ID,
        split=f"{DATASET_SPLIT}[:{NUM_CALIBRATION_SAMPLES}]",
    )

    def preprocess(example):
        return {
            "text": tokenizer.apply_chat_template(
                example["conversations"],
                add_generation_prompt=True,
                tokenize=False
            )
        }

    ds = ds.map(preprocess)
    print(f"[INFO] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ({len(ds)}ê°œ ìƒ˜í”Œ)")

# =========================================================
# 7. GPTQ ì–‘ìí™”
# =========================================================
    print("\n" + "=" * 60)
    print("[INFO] GPTQ ì–‘ìí™” ì‹œì‘ (ì•½ 10~20ë¶„ ì†Œìš”)")
    print(f"       Scheme: {SCHEME}")
    print(f"       ActOrder: {ACTORDER}")
    print(f"       Max Seq Length: {MAX_SEQUENCE_LENGTH}")
    print("=" * 60)

    recipe = [
        GPTQModifier(
            scheme=SCHEME,
            targets=TARGETS,
            ignore=IGNORE,
            actorder=ACTORDER,
            dampening_frac=DAMPENING_FRAC,
            # sequential_targets=["Exaone4DecoderLayer"], # â­ T4 í•„ìˆ˜ ì•ˆì „ì¥ì¹˜ (OOM ë°©ì§€)
        )
    ]

    start_time = time.time()

    oneshot(
        model=model,
        dataset=ds,
        recipe=recipe,
        max_seq_length=MAX_SEQUENCE_LENGTH,
        num_calibration_samples=NUM_CALIBRATION_SAMPLES,
    )

    quantization_time = time.time() - start_time
    print(f"[INFO] GPTQ ì–‘ìí™” ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {quantization_time:.1f}ì´ˆ)")

    # ì–‘ìí™” ì‹œê°„ ê¸°ë¡
    mlflow.log_metric("quantization_time_sec", quantization_time)

# =========================================================
# 8. ëª¨ë¸ ì €ì¥ (Kaggle ê²½ë¡œ)
# =========================================================
    print(f"\n[INFO] ëª¨ë¸ ì €ì¥ ì¤‘... â†’ {OUT_DIR}")

    if os.path.exists(OUT_DIR):
        shutil.rmtree(OUT_DIR)
    os.makedirs(OUT_DIR, exist_ok=True)

    model.save_pretrained(OUT_DIR, save_compressed=True)
    tokenizer.save_pretrained(OUT_DIR)

    # ì €ì¥ í™•ì¸
    print("[INFO] ì €ì¥ëœ íŒŒì¼:")
    for f in os.listdir(OUT_DIR):
        size = os.path.getsize(os.path.join(OUT_DIR, f)) / (1024 * 1024)
        print(f"       - {f} ({size:.1f} MB)")

# =========================================================
# 9. ZIP ìƒì„± (Kaggle Output)
# =========================================================
    zip_name = "kaggle_optimized_submit"
    print(f"\n[INFO] {zip_name}.zip ìƒì„± ì¤‘...")

    # Kaggle Working ë””ë ‰í† ë¦¬ì— ìƒì„±
    shutil.make_archive(
        base_name=f"/kaggle/working/{zip_name}",
        format="zip",
        root_dir="/kaggle/working",
        base_dir="model",
    )

    zip_path = f"/kaggle/working/{zip_name}.zip"
    zip_size = os.path.getsize(zip_path) / (1024 * 1024)
    print(f"[INFO] ìƒì„± ì™„ë£Œ: {zip_path} ({zip_size:.1f} MB)")

    # ZIP íŒŒì¼ í¬ê¸° ê¸°ë¡
    mlflow.log_metric("model_zip_size_MB", zip_size)

    # GPU ì •ë³´ ê¸°ë¡
    if torch.cuda.is_available():
        mlflow.log_param("gpu_count", torch.cuda.device_count())
        mlflow.log_param("gpu_name", torch.cuda.get_device_name(0))
        mlflow.log_metric("gpu_vram_GB", torch.cuda.get_device_properties(0).total_memory / 1e9)

    print("[INFO] MLflow ê¸°ë¡ ì™„ë£Œ!")

    # =========================================================
    # ì™„ë£Œ!
    # =========================================================
    print("\n" + "=" * 60)
    print("âœ… ì–‘ìí™” ì™„ë£Œ!")
    print("=" * 60)
    print(f"""
ğŸ“Š ì„¤ì • ìš”ì•½:
   â€¢ Model: {MODEL_ID}
   â€¢ Scheme: {SCHEME}
   â€¢ ActOrder: {ACTORDER}
   â€¢ Samples: {NUM_CALIBRATION_SAMPLES}
   â€¢ Max Length: {MAX_SEQUENCE_LENGTH}
   â€¢ ì–‘ìí™” ì‹œê°„: {quantization_time:.1f}ì´ˆ

ğŸ“ ì¶œë ¥:
   â€¢ ëª¨ë¸: {OUT_DIR}/
   â€¢ ZIP: {zip_path} ({zip_size:.1f} MB)

ğŸš€ Kaggle ìš°ì¸¡ 'Data' íŒ¨ë„ì˜ Outputì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”!
ğŸ“Š DagsHubì—ì„œ ì‹¤í—˜ ê¸°ë¡ í™•ì¸: https://dagshub.com/sthun0211/LGaimers.mlflow
""")
