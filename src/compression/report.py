"""
EXAONE ê²½ëŸ‰í™” ì‹¤í—˜ ë³´ê³ ì„œ ìë™ ìƒì„±ê¸°

Usage:
    make report
    # ë˜ëŠ”
    uv run python src/compression/report.py --experiment drop28 --model ./submit/model_drop28
"""

import argparse
import json
from datetime import datetime
from pathlib import Path
import sys

# Project Root
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def load_baseline() -> dict:
    """Baseline ê²°ê³¼ ë¡œë“œ"""
    baseline_path = PROJECT_ROOT / "outputs" / "baseline_result.json"
    if baseline_path.exists():
        with open(baseline_path) as f:
            return json.load(f)
    return {
        "perplexity": 2659.93,
        "tokens_per_sec": 27.30,
        "time_per_token_ms": 36.63,
        "num_params": 1_280_000_000,
    }


def load_experiment_result(model_path: str) -> dict:
    """ì‹¤í—˜ ê²°ê³¼ ë¡œë“œ (JSON íŒŒì¼ ë˜ëŠ” ìˆ˜ë™ ì…ë ¥)"""
    result_path = Path(model_path) / "eval_result.json"
    if result_path.exists():
        with open(result_path) as f:
            return json.load(f)
    
    # ê²°ê³¼ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    return {}


def calculate_scores(result: dict, baseline: dict) -> dict:
    """PerfNorm, SpeedNorm, Score ê³„ì‚°"""
    if not result:
        return {"perf_norm": None, "speed_norm": None, "score": None}
    
    perf_norm = baseline["perplexity"] / result.get("perplexity", 1) if result.get("perplexity") else None
    
    base_time = baseline.get("time_per_token_ms", 36.63)
    model_time = result.get("time_per_token_ms", base_time)
    speed_norm = 1 - (model_time / base_time) if base_time else 0
    
    if perf_norm is not None:
        score = max(0.5 * perf_norm + 0.5 * speed_norm, 0)
    else:
        score = None
    
    return {
        "perf_norm": perf_norm,
        "speed_norm": speed_norm,
        "score": score
    }


def format_number(value, precision=4):
    """ìˆ«ì í¬ë§·íŒ… (None ì²˜ë¦¬)"""
    if value is None:
        return "N/A"
    if isinstance(value, float):
        return f"{value:.{precision}f}"
    return str(value)


def format_params(num_params):
    """íŒŒë¼ë¯¸í„° ìˆ˜ í¬ë§·íŒ… (1.28B í˜•ì‹)"""
    if num_params is None:
        return "N/A"
    if num_params >= 1e9:
        return f"{num_params/1e9:.2f}B"
    elif num_params >= 1e6:
        return f"{num_params/1e6:.2f}M"
    return str(num_params)


def generate_report(
    experiment_name: str,
    model_path: str,
    description: str = "",
    conclusion: str = ""
) -> str:
    """ë³´ê³ ì„œ ë§ˆí¬ë‹¤ìš´ ìƒì„±"""
    
    today = datetime.now().strftime("%Y-%m-%d")
    baseline = load_baseline()
    result = load_experiment_result(model_path)
    scores = calculate_scores(result, baseline)
    
    # ì„±ê³µ/ì‹¤íŒ¨ íŒë‹¨
    if scores["score"] is None:
        status = "âš ï¸ í‰ê°€ í•„ìš”"
        status_emoji = "âš ï¸"
    elif scores["score"] >= 0.5:
        status = "âœ… ì„±ê³µ (Baseline ì´ˆê³¼)"
        status_emoji = "âœ…"
    else:
        status = "âŒ ì‹¤íŒ¨ (Baseline ë¯¸ë‹¬)"
        status_emoji = "âŒ"
    
    report = f"""# EXAONE ê²½ëŸ‰í™” ì‹¤í—˜ ë³´ê³ ì„œ

> **ì‘ì„±ì¼**: {today}  
> **ì‹¤í—˜ëª…**: {experiment_name}  
> **ëª¨ë¸ ê²½ë¡œ**: `{model_path}`

---

## 1. ì‹¤í—˜ ê°œìš”

### 1.1 ëª©í‘œ
{description if description else f"{experiment_name} ì „ëµì„ ì ìš©í•˜ì—¬ ëª¨ë¸ ê²½ëŸ‰í™” ë° ì„±ëŠ¥ í‰ê°€"}

### 1.2 ì£¼ìš” ê²°ê³¼ ìš”ì•½
- **ê²°ê³¼**: {status}
- **Score**: {format_number(scores['score'])}
- **PerfNorm**: {format_number(scores['perf_norm'])}
- **SpeedNorm**: {format_number(scores['speed_norm'])}

---

## 2. ì‹¤í—˜ ê²°ê³¼ ë¹„êµ

| ëª¨ë¸ | Params | Tokens/sec | Perplexity | PerfNorm | SpeedNorm | **Score** |
|------|--------|------------|------------|----------|-----------|-----------|
| **Baseline** | {format_params(baseline.get('num_params'))} | {format_number(baseline.get('tokens_per_sec'), 2)} | {format_number(baseline.get('perplexity'), 2)} | 1.0000 | 0.0000 | **0.5000** |
| **{experiment_name}** | {format_params(result.get('num_params'))} | {format_number(result.get('tokens_per_sec'), 2)} | {format_number(result.get('perplexity'), 2)} | {format_number(scores['perf_norm'])} | {format_number(scores['speed_norm'])} | **{format_number(scores['score'])}** |

---

## 3. ë¶„ì„

### 3.1 ì†ë„ ë³€í™”
- Baseline ëŒ€ë¹„ SpeedNorm: **{format_number(scores['speed_norm'])}**
- {"ì†ë„ê°€ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤." if scores['speed_norm'] and scores['speed_norm'] > 0 else "ì†ë„ ê°œì„ ì´ ë¯¸ë¯¸í•˜ê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤."}

### 3.2 ì„±ëŠ¥ ë³€í™”  
- Baseline ëŒ€ë¹„ PerfNorm: **{format_number(scores['perf_norm'])}**
- {"ì„±ëŠ¥ì´ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤." if scores['perf_norm'] and scores['perf_norm'] >= 0.9 else "ì„±ëŠ¥ ì†ì‹¤ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤." if scores['perf_norm'] else "ì„±ëŠ¥ í‰ê°€ê°€ í•„ìš”í•©ë‹ˆë‹¤."}

---

## 4. ê²°ë¡  ë° ì œì•ˆ

{conclusion if conclusion else f'''
### {status_emoji} ê²°ë¡ 
{"ì´ ì „ëµì€ Baseline Score(0.5)ë¥¼ ì´ˆê³¼í•˜ì—¬ **ì œì¶œ ê°€ëŠ¥**í•©ë‹ˆë‹¤." if scores['score'] and scores['score'] >= 0.5 else "ì´ ì „ëµì€ Baseline Score(0.5)ì— ë¯¸ë‹¬í•˜ì—¬ **ì œì¶œ ë¹„ê¶Œì¥**ì…ë‹ˆë‹¤." if scores['score'] else "í‰ê°€ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ ì œì¶œ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ì„¸ìš”."}

### ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„
1. `make eval-model` ë¡œ í‰ê°€ ì‹¤í–‰ (ì•„ì§ ì•ˆ í–ˆë‹¤ë©´)
2. MLflowì—ì„œ ê²°ê³¼ í™•ì¸
3. ì œì¶œ ì—¬ë¶€ ê²°ì •
'''}
"""
    return report


def save_report(report: str, experiment_name: str):
    """ë³´ê³ ì„œ ì €ì¥"""
    today = datetime.now().strftime("%Y-%m-%d")
    output_dir = PROJECT_ROOT / "outputs" / f"{today}_{experiment_name}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    report_path = output_dir / f"{today}_report.md"
    with open(report_path, "w") as f:
        f.write(report)
    
    print(f"âœ… ë³´ê³ ì„œ ì €ì¥: {report_path}")
    return report_path


def main():
    parser = argparse.ArgumentParser(description="EXAONE ê²½ëŸ‰í™” ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„±")
    parser.add_argument("--experiment", "-e", type=str, default="experiment",
                        help="ì‹¤í—˜ ì´ë¦„ (ì˜ˆ: drop28, fp16)")
    parser.add_argument("--model", "-m", type=str, default="./submit/model",
                        help="ê²½ëŸ‰í™” ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--description", "-d", type=str, default="",
                        help="ì‹¤í—˜ ì„¤ëª…")
    parser.add_argument("--conclusion", "-c", type=str, default="",
                        help="ê²°ë¡  (ì§ì ‘ ì…ë ¥)")
    parser.add_argument("--print-only", action="store_true",
                        help="íŒŒì¼ ì €ì¥ ì—†ì´ ì¶œë ¥ë§Œ")
    
    args = parser.parse_args()
    
    print(f"ğŸ“ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
    print(f"  - ì‹¤í—˜: {args.experiment}")
    print(f"  - ëª¨ë¸: {args.model}")
    
    report = generate_report(
        experiment_name=args.experiment,
        model_path=args.model,
        description=args.description,
        conclusion=args.conclusion
    )
    
    if args.print_only:
        print("\n" + "="*60)
        print(report)
    else:
        save_report(report, args.experiment)
        print("\nğŸ’¡ Tip: ë” ìƒì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ë©´ AIì—ê²Œ ìš”ì²­í•˜ì„¸ìš”!")


if __name__ == "__main__":
    main()
