"""
LG Aimers - ë¡œì»¬ ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ëª©ì : ì–‘ìí™”ëœ ëª¨ë¸ë“¤ì„ ê¸°ë³¸ ëª¨ë¸(EXAONE-4.0-1.2B) ëŒ€ë¹„ ë¹„êµ í‰ê°€í•˜ì—¬
      ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì„ ì„ ë³„í•œ í›„ ëŒ€íšŒ ì„œë²„ì— ì œì¶œ

í‰ê°€ ì‚°ì‹:
  Score = max(0.5 Ã— PerfNorm + 0.5 Ã— SpeedNorm, 0)
  - PerfNorm  = ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì •í™•ë„ / ê¸°ë³¸ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì •í™•ë„
  - SpeedNorm = 1 - (ëª¨ë¸ í† í°ë‹¹ ì‹œê°„) / (ê¸°ë³¸ ëª¨ë¸ í† í°ë‹¹ ì‹œê°„)

PerfNorm ì¸¡ì •:
  lm-evaluation-harnessë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ë²¤ì¹˜ë§ˆí¬ (MMLU, GSM8K ë“±)

SpeedNorm ì¸¡ì •:
  ë™ì¼ í™˜ê²½ì—ì„œì˜ í† í° ìƒì„± ì†ë„ ìƒëŒ€ ë¹„êµ

ì‚¬ì „ ì„¤ì¹˜:
  pip install lm-eval torch transformers accelerate safetensors

ì‚¬ìš©ë²•:
  # ê¸°ë³¸ ëª¨ë¸ baseline ì¸¡ì • (ìµœì´ˆ 1íšŒ)
  python evaluate_local.py --base-model ./base_model --mode baseline

  # ì–‘ìí™” ëª¨ë¸ í‰ê°€ (ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ë¹„êµ)
  python evaluate_local.py --base-model ./base_model --target-model ./model_DB/optimized_submit/model

  # ì—¬ëŸ¬ ëª¨ë¸ í•œë²ˆì— ë¹„êµ
  python evaluate_local.py --base-model ./base_model --target-model ./modelA ./modelB ./modelC

  # ë²¤ì¹˜ë§ˆí¬ íƒœìŠ¤í¬ ì§€ì • (ê¸°ë³¸: gsm8k,mmlu)
  python evaluate_local.py --base-model ./base_model --target-model ./model --tasks gsm8k,mmlu

  # ì†ë„ ì¸¡ì • ìƒëµ (ì •í™•ë„ë§Œ ë¹„êµ)
  python evaluate_local.py --base-model ./base_model --target-model ./model --skip-speed

  # ì´ì „ baseline ê²°ê³¼ ì¬ì‚¬ìš© (ì‹œê°„ ì ˆì•½)
  python evaluate_local.py --target-model ./model --baseline-json ./baseline_result.json
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from dataclasses import dataclass, asdict, field
from typing import Optional, List, Dict

import torch


# =========================================================
# ë°ì´í„° í´ë˜ìŠ¤
# =========================================================

@dataclass
class ModelResult:
    """ë‹¨ì¼ ëª¨ë¸ í‰ê°€ ê²°ê³¼"""
    model_path: str
    # ë²¤ì¹˜ë§ˆí¬ ì •í™•ë„ (PerfNorm ì‚°ì¶œìš©)
    benchmark_scores: Dict[str, float] = field(default_factory=dict)
    avg_accuracy: float = 0.0
    # ì†ë„ (SpeedNorm ì‚°ì¶œìš©)
    time_per_token_ms: float = 0.0
    tokens_per_sec: float = 0.0
    total_tokens: int = 0
    total_time_sec: float = 0.0
    # ëª¨ë¸ ì •ë³´
    num_parameters: int = 0
    model_size_mb: float = 0.0


@dataclass
class ComparisonEntry:
    """ëª¨ë¸ ê°„ ë¹„êµ ê²°ê³¼ (í•œ ì¤„)"""
    model_path: str
    avg_accuracy: float
    perf_norm: float
    time_per_token_ms: float
    speed_norm: float
    score: float
    benchmark_details: Dict[str, float] = field(default_factory=dict)


# =========================================================
# 1. ë²¤ì¹˜ë§ˆí¬ í‰ê°€ (PerfNormìš©) - lm-evaluation-harness ì‚¬ìš©
# =========================================================

def run_benchmarks(model_path: str, tasks: List[str], 
                   batch_size: str = "auto", num_fewshot: int = None) -> Dict[str, float]:
    """
    lm-evaluation-harnessë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¤ì¹˜ë§ˆí¬ ì •í™•ë„ ì¸¡ì •
    
    Returns:
        Dict[task_name, accuracy]  (0.0 ~ 1.0)
    """
    import lm_eval

    print(f"\n  ğŸ“Š ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹œì‘: {', '.join(tasks)}")
    print(f"     ëª¨ë¸: {model_path}")
    
    model_args = f"pretrained={model_path},trust_remote_code=True"
    
    # GPU VRAM ë¶€ì¡± ì‹œ dtype ì§€ì •
    if torch.cuda.is_available():
        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        if vram_gb < 12:
            model_args += ",dtype=float16"
    
    results = lm_eval.simple_evaluate(
        model="hf",
        model_args=model_args,
        tasks=tasks,
        batch_size=batch_size,
        num_fewshot=num_fewshot,
    )
    
    # íƒœìŠ¤í¬ë³„ ì •í™•ë„ ì¶”ì¶œ
    scores = {}
    for task_name in tasks:
        task_result = results["results"].get(task_name, {})
        
        # lm-evalì€ íƒœìŠ¤í¬ì— ë”°ë¼ ë‹¤ë¥¸ metricëª…ì„ ì‚¬ìš©
        # ìš°ì„ ìˆœìœ„: acc_norm > acc > exact_match
        acc = None
        for metric_key in ["acc_norm,none", "acc,none", "exact_match,none",
                           "acc_norm", "acc", "exact_match"]:
            if metric_key in task_result:
                acc = task_result[metric_key]
                break
        
        if acc is not None:
            scores[task_name] = acc
            print(f"     âœ… {task_name}: {acc:.4f} ({acc*100:.2f}%)")
        else:
            # í•˜ìœ„ íƒœìŠ¤í¬ê°€ ìˆëŠ” ê²½ìš° (ì˜ˆ: mmluëŠ” ì—¬ëŸ¬ subject)
            # ê·¸ë£¹ í‰ê·  ì°¾ê¸°
            for key, val in task_result.items():
                if "acc" in key and isinstance(val, (int, float)):
                    scores[task_name] = val
                    print(f"     âœ… {task_name}: {val:.4f} ({val*100:.2f}%)")
                    break
            else:
                print(f"     âš ï¸ {task_name}: ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (ê±´ë„ˆëœë‹ˆë‹¤)")
                print(f"        ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {list(task_result.keys())}")
    
    return scores


# =========================================================
# 2. ì†ë„ í‰ê°€ (SpeedNormìš©) - HF generate() ìƒëŒ€ ë¹„êµ
# =========================================================

SPEED_PROMPTS = [
    "Explain the concept of machine learning in simple terms.",
    "What are the benefits of renewable energy?",
    "Write a short paragraph about artificial intelligence.",
    "Describe the process of photosynthesis.",
    "What is the capital of France and why is it famous?",
    "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "í•œêµ­ì˜ ì „í†µ ìŒì‹ ì¤‘ í•˜ë‚˜ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.",
    "í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ëŠ” ì¢‹ì€ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "Solve: If a train travels 60km/h for 2 hours, how far?",
    "What is the difference between a stack and a queue?",
    "ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "Write a Python function to reverse a string.",
]


def measure_speed(model_path: str, max_new_tokens: int = 128) -> Dict:
    """
    HuggingFace model.generate()ë¡œ í† í° ìƒì„± ì†ë„ ì¸¡ì •
    (ìƒëŒ€ ë¹„êµ ëª©ì , ì ˆëŒ€ ìˆ˜ì¹˜ëŠ” ëŒ€íšŒì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    """
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    print(f"\n  â±ï¸  ì†ë„ ì¸¡ì • ì‹œì‘ ({len(SPEED_PROMPTS)}ê°œ í”„ë¡¬í”„íŠ¸, max_tokens={max_new_tokens})")
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, trust_remote_code=True,
        local_files_only=os.path.isdir(model_path),
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True,
        local_files_only=os.path.isdir(model_path),
    )
    model.eval()
    
    # ì›Œë°ì—…
    warmup = tokenizer("Hello", return_tensors="pt").to(device)
    with torch.no_grad():
        _ = model.generate(**warmup, max_new_tokens=5)
    if device == "cuda":
        torch.cuda.synchronize()
    
    total_tokens = 0
    total_time = 0.0
    
    for prompt in SPEED_PROMPTS:
        messages = [{"role": "user", "content": prompt}]
        try:
            input_ids = tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
            ).to(device)
        except Exception:
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            input_ids = inputs["input_ids"]
        
        if device == "cuda":
            torch.cuda.synchronize()
        
        start = time.perf_counter()
        with torch.no_grad():
            output = model.generate(
                input_ids, max_new_tokens=max_new_tokens,
                do_sample=False, pad_token_id=tokenizer.eos_token_id
            )
        if device == "cuda":
            torch.cuda.synchronize()
        
        elapsed = time.perf_counter() - start
        new_tokens = output.shape[1] - input_ids.shape[1]
        total_tokens += new_tokens
        total_time += elapsed
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    if device == "cuda":
        torch.cuda.empty_cache()
    
    result = {
        "total_time_sec": total_time,
        "total_tokens": total_tokens,
        "tokens_per_sec": total_tokens / total_time if total_time > 0 else 0,
        "time_per_token_ms": (total_time / total_tokens * 1000) if total_tokens > 0 else 0,
    }
    
    print(f"     Tokens/sec: {result['tokens_per_sec']:.2f}")
    print(f"     Time/token: {result['time_per_token_ms']:.2f} ms")
    
    return result


# =========================================================
# 3. ì „ì²´ í‰ê°€ + ì ìˆ˜ ê³„ì‚°
# =========================================================

def evaluate_model(model_path: str, tasks: List[str],
                   skip_speed: bool = False, max_new_tokens: int = 128) -> ModelResult:
    """ë‹¨ì¼ ëª¨ë¸ ì „ì²´ í‰ê°€"""
    
    print(f"\n{'â”' * 60}")
    print(f"  ğŸ“Œ í‰ê°€ ëª¨ë¸: {model_path}")
    print(f"{'â”' * 60}")
    
    # ëª¨ë¸ í¬ê¸° í™•ì¸
    model_dir = Path(model_path)
    model_size_mb = 0
    if model_dir.is_dir():
        for f in model_dir.glob("*.safetensors"):
            model_size_mb += f.stat().st_size / (1024 * 1024)
        for f in model_dir.glob("*.bin"):
            model_size_mb += f.stat().st_size / (1024 * 1024)
        print(f"  ëª¨ë¸ ê°€ì¤‘ì¹˜ í¬ê¸°: {model_size_mb:.1f} MB")
    
    # ë²¤ì¹˜ë§ˆí¬ í‰ê°€
    scores = run_benchmarks(model_path, tasks)
    avg_acc = sum(scores.values()) / len(scores) if scores else 0.0
    print(f"  ğŸ“Š í‰ê·  ì •í™•ë„: {avg_acc:.4f} ({avg_acc*100:.2f}%)")
    
    # ì†ë„ ì¸¡ì •
    speed = {"time_per_token_ms": 0, "tokens_per_sec": 0, "total_tokens": 0, "total_time_sec": 0}
    if not skip_speed:
        # ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ í›„ ì†ë„ ì¸¡ì •
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        speed = measure_speed(model_path, max_new_tokens)
    
    return ModelResult(
        model_path=model_path,
        benchmark_scores=scores,
        avg_accuracy=avg_acc,
        time_per_token_ms=speed["time_per_token_ms"],
        tokens_per_sec=speed["tokens_per_sec"],
        total_tokens=speed["total_tokens"],
        total_time_sec=speed["total_time_sec"],
        model_size_mb=model_size_mb,
    )


def calculate_score(base: ModelResult, target: ModelResult, skip_speed: bool = False) -> ComparisonEntry:
    """ëŒ€íšŒ ì‚°ì‹ì— ë”°ë¥¸ ì ìˆ˜ ê³„ì‚°"""
    
    # PerfNorm = target ì •í™•ë„ / base ì •í™•ë„
    if base.avg_accuracy > 0:
        perf_norm = target.avg_accuracy / base.avg_accuracy
    else:
        perf_norm = 1.0
    
    # SpeedNorm = 1 - (target time/token) / (base time/token)
    if not skip_speed and base.time_per_token_ms > 0 and target.time_per_token_ms > 0:
        speed_norm = 1 - (target.time_per_token_ms / base.time_per_token_ms)
    else:
        speed_norm = 0.0  # ì†ë„ ë¯¸ì¸¡ì • ì‹œ 0ìœ¼ë¡œ ì²˜ë¦¬
    
    # Score
    score = max(0.5 * perf_norm + 0.5 * speed_norm, 0)
    
    return ComparisonEntry(
        model_path=target.model_path,
        avg_accuracy=target.avg_accuracy,
        perf_norm=perf_norm,
        time_per_token_ms=target.time_per_token_ms,
        speed_norm=speed_norm,
        score=score,
        benchmark_details=target.benchmark_scores,
    )


# =========================================================
# 4. ê²°ê³¼ ì¶œë ¥
# =========================================================

def print_comparison(base: ModelResult, entries: List[ComparisonEntry], skip_speed: bool):
    """ê° ëª¨ë¸ì„ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ê°œë³„ ì¶œë ¥"""
    
    print("\n\n" + "=" * 80)
    print("  ğŸ† LG Aimers ë¡œì»¬ í‰ê°€ ê²°ê³¼ (lm-evaluation-harness ê¸°ë°˜)")
    print("=" * 80)
    
    # ê¸°ë³¸ ëª¨ë¸ (ê¸°ì¤€)
    base_name = Path(base.model_path).name or "base"
    print(f"\nğŸ“‹ ê¸°ì¤€ ëª¨ë¸: {base_name}")
    print(f"{'â”€' * 80}")
    print(f"  ê²½ë¡œ:       {base.model_path}")
    print(f"  í‰ê·  ì •í™•ë„: {base.avg_accuracy:.4f} ({base.avg_accuracy*100:.2f}%)")
    for task, score in base.benchmark_scores.items():
        print(f"    - {task}: {score:.4f}")
    if not skip_speed:
        print(f"  Time/token: {base.time_per_token_ms:.2f} ms")
    print(f"  â†’ ì´ ëª¨ë¸ì´ PerfNorm=1.0, SpeedNorm=0.0, Score=0.5 ì˜ ê¸°ì¤€ì…ë‹ˆë‹¤.")
    
    # â”€â”€ ê° ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ë¹„êµ â”€â”€
    entries_sorted = sorted(entries, key=lambda x: x.score, reverse=True)
    tasks = list(base.benchmark_scores.keys())
    
    for idx, e in enumerate(entries_sorted, 1):
        model_name = Path(e.model_path).name or e.model_path
        
        print(f"\n\n{'â”' * 80}")
        print(f"  ğŸ“Œ [{idx}] {model_name} / ê¸°ì¤€ ëª¨ë¸ ë¹„êµ")
        print(f"{'â”' * 80}")
        
        # íƒœìŠ¤í¬ë³„ ë¹„ìœ¨
        print(f"\n  ğŸ¯ PerfNorm (ë²¤ì¹˜ë§ˆí¬ ì •í™•ë„ ë¹„ìœ¨)")
        print(f"  {'â”€' * 60}")
        print(f"  {'íƒœìŠ¤í¬':<15} {'ê¸°ì¤€ ëª¨ë¸':<12} {'ì´ ëª¨ë¸':<12} {'ë¹„ìœ¨ (ëª¨ë¸/ê¸°ì¤€)':<18}")
        print(f"  {'â”€' * 60}")
        
        for task in tasks:
            base_score = base.benchmark_scores.get(task, 0)
            target_score = e.benchmark_details.get(task, 0)
            ratio = target_score / base_score if base_score > 0 else 0
            arrow = "âœ…" if ratio >= 0.95 else ("âš ï¸" if ratio >= 0.85 else "âŒ")
            print(f"  {task:<15} {base_score:.4f}       {target_score:.4f}       {ratio:.4f} ({ratio*100:.1f}%)  {arrow}")
        
        # í‰ê· 
        print(f"  {'â”€' * 60}")
        print(f"  {'í‰ê· ':<15} {base.avg_accuracy:.4f}       {e.avg_accuracy:.4f}       {e.perf_norm:.4f} ({e.perf_norm*100:.1f}%)")
        print(f"\n  â†’ PerfNorm = {e.avg_accuracy:.4f} / {base.avg_accuracy:.4f} = {e.perf_norm:.4f}")
        
        # ì†ë„ ë¹„ìœ¨
        if not skip_speed:
            print(f"\n  â±ï¸  SpeedNorm (í† í°ë‹¹ ì¶”ë¡  ì‹œê°„ ë¹„ìœ¨)")
            print(f"  {'â”€' * 60}")
            print(f"  ê¸°ì¤€ ëª¨ë¸ Time/token:  {base.time_per_token_ms:.2f} ms")
            print(f"  ì´ ëª¨ë¸ Time/token:    {e.time_per_token_ms:.2f} ms")
            time_ratio = e.time_per_token_ms / base.time_per_token_ms if base.time_per_token_ms > 0 else 1
            print(f"  ì‹œê°„ ë¹„ìœ¨:             {time_ratio:.4f} ({time_ratio*100:.1f}%)")
            speed_arrow = "âœ… ë¹¨ë¼ì§" if e.speed_norm > 0 else ("âš¡ ë™ì¼" if e.speed_norm == 0 else "âŒ ëŠë ¤ì§")
            print(f"\n  â†’ SpeedNorm = 1 - {e.time_per_token_ms:.2f} / {base.time_per_token_ms:.2f} = {e.speed_norm:+.4f}  {speed_arrow}")
        
        # ìµœì¢… Score
        print(f"\n  ğŸ† ìµœì¢… Score")
        print(f"  {'â”€' * 60}")
        print(f"  Score = max(0.5 Ã— PerfNorm + 0.5 Ã— SpeedNorm, 0)")
        print(f"        = max(0.5 Ã— {e.perf_norm:.4f} + 0.5 Ã— {e.speed_norm:+.4f}, 0)")
        print(f"        = {e.score:.4f}")
        
        if e.score > 0.5:
            print(f"\n  âœ… ìˆ˜ë£Œ ê¸°ì¤€ (> 0.5) í†µê³¼!  (ê¸°ì¤€ ëŒ€ë¹„ +{e.score - 0.5:.4f})")
        else:
            print(f"\n  âŒ ìˆ˜ë£Œ ê¸°ì¤€ (> 0.5) ë¯¸ë‹¬  (ë¶€ì¡±ë¶„: {0.5 - e.score:.4f})")
    
    # â”€â”€ ìµœì¢… ìš”ì•½ ìˆœìœ„ â”€â”€
    print(f"\n\n{'=' * 80}")
    print(f"  ğŸ“Š ìµœì¢… ìˆœìœ„ ìš”ì•½ (ëª¨ë“  ëª¨ë¸ / ê¸°ì¤€ ëª¨ë¸ ë¹„êµ)")
    print(f"{'=' * 80}")
    print(f"  {'ìˆœìœ„':<4} {'ëª¨ë¸':<28} {'PerfNorm':<10} {'SpeedNorm':<11} {'Score':<8} {'íŒì •'}")
    print(f"{'â”€' * 80}")
    
    for i, e in enumerate(entries_sorted, 1):
        name = Path(e.model_path).name or e.model_path
        if len(name) > 26:
            name = name[:23] + "..."
        verdict = "âœ… í†µê³¼" if e.score > 0.5 else "âŒ ë¯¸ë‹¬"
        star = " â­ BEST" if i == 1 else ""
        print(f"  {i:<4} {name:<28} {e.perf_norm:.4f}    {e.speed_norm:+.4f}    {e.score:.4f}  {verdict}{star}")
    
    print(f"{'â”€' * 80}")
    print(f"  ref  {'ê¸°ì¤€(EXAONE-4.0-1.2B)':<28} 1.0000    +0.0000    0.5000  ê¸°ì¤€ì„ ")
    print(f"{'=' * 80}")
    
    if skip_speed:
        print(f"  âš ï¸  SpeedNorm ë¯¸ì¸¡ì •: ì‹¤ì œ ScoreëŠ” ì†ë„ ê°œì„ ë¶„ë§Œí¼ ë” ë†’ì„ ìˆ˜ ìˆìŒ")
    print(f"  âš ï¸  PerfNormì€ ê³µê°œ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ì´ë©° ëŒ€íšŒ ë¹„ê³µê°œ ë²¤ì¹˜ì…‹ê³¼ ì°¨ì´ ê°€ëŠ¥")
    
    return entries_sorted


def save_result(base: ModelResult, entries: List[ComparisonEntry], output_path: str):
    """ê²°ê³¼ JSON ì €ì¥"""
    data = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "baseline": asdict(base),
        "models": [asdict(e) for e in entries],
        "ranking": [
            {"rank": i+1, "model": Path(e.model_path).name, "score": e.score}
            for i, e in enumerate(sorted(entries, key=lambda x: x.score, reverse=True))
        ],
    }
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")


# =========================================================
# ë©”ì¸
# =========================================================

def main():
    parser = argparse.ArgumentParser(
        description="LG Aimers ë¡œì»¬ ëª¨ë¸ í‰ê°€ (lm-evaluation-harness ê¸°ë°˜)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # 1. baseline ì¸¡ì • + ì €ì¥
  python evaluate_local.py --base-model ./base_model --mode baseline

  # 2. ì–‘ìí™” ëª¨ë¸ 1ê°œ í‰ê°€
  python evaluate_local.py --base-model ./base_model --target-model ./model_DB/optimized_submit/model

  # 3. ì—¬ëŸ¬ ëª¨ë¸ í•œë²ˆì— ë¹„êµ
  python evaluate_local.py --base-model ./base_model --target-model ./modelA ./modelB ./modelC

  # 4. ì €ì¥ëœ baseline ì¬ì‚¬ìš© (ì‹œê°„ ì ˆì•½)
  python evaluate_local.py --baseline-json ./baseline_result.json --target-model ./modelA

  # 5. ì •í™•ë„ë§Œ ë¹„êµ (ì†ë„ ìƒëµ)
  python evaluate_local.py --base-model ./base_model --target-model ./model --skip-speed
        """
    )
    
    parser.add_argument("--base-model", type=str, default=None,
                        help="ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ (EXAONE-4.0-1.2B)")
    parser.add_argument("--target-model", type=str, nargs="+", default=None,
                        help="í‰ê°€í•  ì–‘ìí™” ëª¨ë¸ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)")
    parser.add_argument("--mode", choices=["baseline", "compare"], default="compare",
                        help="baseline: ê¸°ë³¸ ëª¨ë¸ë§Œ ì¸¡ì • / compare: ë¹„êµ í‰ê°€")
    parser.add_argument("--tasks", type=str, default="gsm8k,mmlu",
                        help="ë²¤ì¹˜ë§ˆí¬ íƒœìŠ¤í¬ (ì‰¼í‘œ êµ¬ë¶„, ê¸°ë³¸: gsm8k,mmlu)")
    parser.add_argument("--skip-speed", action="store_true",
                        help="ì†ë„ ì¸¡ì • ìƒëµ (ì •í™•ë„ë§Œ ë¹„êµ)")
    parser.add_argument("--max-tokens", type=int, default=128,
                        help="ì†ë„ ì¸¡ì • ì‹œ ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--baseline-json", type=str, default=None,
                        help="ì´ì „ì— ì €ì¥í•œ baseline ê²°ê³¼ JSON ê²½ë¡œ (ì¬ì¸¡ì • ìƒëµ)")
    parser.add_argument("--output", type=str, default=None,
                        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: ìë™ ìƒì„±)")
    
    args = parser.parse_args()
    tasks = [t.strip() for t in args.tasks.split(",")]
    
    print("\n" + "=" * 80)
    print("  LG Aimers ë¡œì»¬ ëª¨ë¸ í‰ê°€ (lm-evaluation-harness ê¸°ë°˜)")
    print("=" * 80)
    print(f"  ë²¤ì¹˜ë§ˆí¬: {', '.join(tasks)}")
    print(f"  ì†ë„ ì¸¡ì •: {'ìƒëµ' if args.skip_speed else 'ì‹¤í–‰'}")
    
    # â”€â”€â”€ Baseline ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    base_result = None
    
    if args.baseline_json:
        # ì €ì¥ëœ baseline ë¡œë“œ
        print(f"\n  ğŸ“‚ Baseline ë¡œë“œ: {args.baseline_json}")
        with open(args.baseline_json, "r") as f:
            data = json.load(f)
        base_data = data if "benchmark_scores" in data else data.get("baseline", data)
        base_result = ModelResult(**{k: v for k, v in base_data.items() if k in ModelResult.__dataclass_fields__})
        print(f"     í‰ê·  ì •í™•ë„: {base_result.avg_accuracy:.4f}")
    
    elif args.base_model:
        # ê¸°ë³¸ ëª¨ë¸ í‰ê°€
        base_result = evaluate_model(args.base_model, tasks, args.skip_speed, args.max_tokens)
        
        # Baseline ê²°ê³¼ ì €ì¥
        baseline_path = "baseline_result.json"
        save_result(base_result, [], baseline_path)
        print(f"  ğŸ’¾ Baseline ì €ì¥ë¨ â†’ ë‹¤ìŒë¶€í„° --baseline-json {baseline_path} ë¡œ ì¬ì‚¬ìš© ê°€ëŠ¥")
    
    if args.mode == "baseline":
        if base_result:
            print(f"\nâœ… Baseline ì¸¡ì • ì™„ë£Œ!")
            print(f"   í‰ê·  ì •í™•ë„: {base_result.avg_accuracy:.4f}")
            for t, s in base_result.benchmark_scores.items():
                print(f"   - {t}: {s:.4f}")
        else:
            print("âŒ --base-model ì„ ì§€ì •í•´ì£¼ì„¸ìš”")
        return
    
    # â”€â”€â”€ Target ëª¨ë¸ í‰ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not args.target_model:
        print("âŒ --target-model ì„ ì§€ì •í•´ì£¼ì„¸ìš”")
        return
    
    if base_result is None:
        print("âŒ --base-model ë˜ëŠ” --baseline-json ì„ ì§€ì •í•´ì£¼ì„¸ìš”")
        return
    
    entries = []
    for model_path in args.target_model:
        target_result = evaluate_model(model_path, tasks, args.skip_speed, args.max_tokens)
        entry = calculate_score(base_result, target_result, args.skip_speed)
        entries.append(entry)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # â”€â”€â”€ ê²°ê³¼ ì¶œë ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    sorted_entries = print_comparison(base_result, entries, args.skip_speed)
    
    # â”€â”€â”€ ê²°ê³¼ ì €ì¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if args.output is None:
        output_path = f"eval_comparison_{time.strftime('%Y%m%d_%H%M%S')}.json"
    else:
        output_path = args.output
    
    save_result(base_result, sorted_entries, output_path)


if __name__ == "__main__":
    main()
