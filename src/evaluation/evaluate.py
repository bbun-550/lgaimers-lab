"""
EXAONE ëª¨ë¸ ì„±ëŠ¥/ì†ë„ í‰ê°€

í‰ê°€ ì§€í‘œ:
1. PerfNorm = Perf_model / Perf_base_model (ì„±ëŠ¥ ìœ ì§€ ë¹„ìœ¨)
2. SpeedNorm = 1 - (Time/Tokens)_model / (Time/Tokens)_base (ì†ë„ ê°œì„  ë¹„ìœ¨)
3. Score = max(0.5 * PerfNorm + 0.5 * SpeedNorm, 0)

MLflowë¡œ ëª¨ë“  ì‹¤í—˜ ê¸°ë¡ (DagsHub ì—°ë™)
"""

import os
import time
import json
import torch
import mlflow
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (MLflow ì¸ì¦ ì •ë³´)
load_dotenv()

# DagsHub MLflow ì„¤ì •
if os.getenv("MLFLOW_TRACKING_URI"):
    mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))
    print(f"ğŸ“¡ MLflow tracking: {os.getenv('MLFLOW_TRACKING_URI')}")
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, asdict
from transformers import AutoModelForCausalLM, AutoTokenizer


@dataclass
class EvalResult:
    """í‰ê°€ ê²°ê³¼"""
    model_name: str
    num_samples: int
    total_time_sec: float
    total_tokens: int
    tokens_per_sec: float
    time_per_token_ms: float
    # ì„±ëŠ¥ ì§€í‘œ (perplexity ë“±)
    perplexity: Optional[float] = None
    # ê²½ëŸ‰í™” ë¹„êµìš©
    perf_norm: Optional[float] = None
    speed_norm: Optional[float] = None
    score: Optional[float] = None


def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.backends.mps.is_available():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    return "cpu"


def load_model(model_path: str, device: str = None):
    """ëª¨ë¸ ë¡œë“œ"""
    if device is None:
        device = get_device()
    
    print(f"Loading model from: {model_path}")
    print(f"Device: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    if device == "mps":
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        ).to(device)
    else:
        try:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
        except ValueError:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            ).to(device)
    
    model.eval()
    return model, tokenizer, device


def get_test_prompts() -> list[str]:
    """í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ëª©ë¡"""
    return [
        "Explain the concept of machine learning in simple terms.",
        "What are the benefits of renewable energy?",
        "Write a short poem about the ocean.",
        "Describe the process of photosynthesis.",
        "What is the capital of France and why is it famous?",
        "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "í•œêµ­ì˜ ì „í†µ ìŒì‹ ì¤‘ í•˜ë‚˜ë¥¼ ì†Œê°œí•´ì£¼ì„¸ìš”.",
        "í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ëŠ” ì¢‹ì€ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    ]


def evaluate_speed(model, tokenizer, device: str, 
                   prompts: list[str], max_new_tokens: int = 64) -> dict:
    """ì†ë„ í‰ê°€: í† í° ìƒì„± ì‹œê°„ ì¸¡ì •"""
    total_tokens = 0
    total_time = 0.0
    
    # ì›Œë°ì—… (ì²« ì‹¤í–‰ì€ ëŠë¦´ ìˆ˜ ìˆìŒ)
    warmup_input = tokenizer("Hello", return_tensors="pt").to(device)
    with torch.no_grad():
        _ = model.generate(**warmup_input, max_new_tokens=5)
    
    for prompt in prompts:
        messages = [{"role": "user", "content": prompt}]
        
        input_ids = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(device)
        
        # ì‹œê°„ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        start_time = time.perf_counter()
        
        with torch.no_grad():
            output = model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
        
        if device == "cuda":
            torch.cuda.synchronize()
        
        elapsed = time.perf_counter() - start_time
        
        # ìƒì„±ëœ í† í° ìˆ˜ (ì…ë ¥ ì œì™¸)
        new_tokens = output.shape[1] - input_ids.shape[1]
        total_tokens += new_tokens
        total_time += elapsed
    
    return {
        "total_time_sec": total_time,
        "total_tokens": total_tokens,
        "tokens_per_sec": total_tokens / total_time if total_time > 0 else 0,
        "time_per_token_ms": (total_time / total_tokens * 1000) if total_tokens > 0 else 0
    }


def evaluate_perplexity(model, tokenizer, device: str, 
                        texts: list[str] = None) -> float:
    """ì„±ëŠ¥ í‰ê°€: Perplexity ê³„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)"""
    if texts is None:
        texts = [
            "The quick brown fox jumps over the lazy dog.",
            "Machine learning is a subset of artificial intelligence.",
            "ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        ]
    
    total_loss = 0.0
    total_tokens = 0
    
    for text in texts:
        inputs = tokenizer(text, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss.item()
        
        # í† í° ìˆ˜ ê°€ì¤‘ í‰ê· 
        num_tokens = inputs["input_ids"].shape[1]
        total_loss += loss * num_tokens
        total_tokens += num_tokens
    
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss)).item()
    
    return perplexity


def calculate_score(result: EvalResult, baseline: EvalResult = None) -> EvalResult:
    """ìµœì¢… ì ìˆ˜ ê³„ì‚°"""
    if baseline is None:
        # Baselineì´ë©´ ê¸°ì¤€ê°’ ì„¤ì •
        result.perf_norm = 1.0
        result.speed_norm = 0.0  # ì†ë„ ê°œì„  ì—†ìŒ
        result.score = 0.5  # 0.5 * 1.0 + 0.5 * 0.0
    else:
        # PerfNorm: ì„±ëŠ¥ ë¹„ìœ¨ (perplexityëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ ì—­ìˆ˜)
        # ì‹¤ì œë¡œëŠ” ì •í™•ë„ ê¸°ë°˜ì´ì§€ë§Œ, perplexityë¡œ ëŒ€ì²´
        result.perf_norm = baseline.perplexity / result.perplexity if result.perplexity else 1.0
        
        # SpeedNorm: ì†ë„ ê°œì„  ë¹„ìœ¨
        base_time_per_token = baseline.time_per_token_ms
        model_time_per_token = result.time_per_token_ms
        result.speed_norm = 1 - (model_time_per_token / base_time_per_token) if base_time_per_token else 0.0
        
        # Score
        result.score = max(0.5 * result.perf_norm + 0.5 * result.speed_norm, 0)
    
    return result


def evaluate_model(model_path: str, 
                   experiment_name: str = "exaone_compression",
                   run_name: str = None,
                   baseline_result: EvalResult = None,
                   max_new_tokens: int = 64) -> EvalResult:
    """ëª¨ë¸ ì¢…í•© í‰ê°€"""
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer, device = load_model(model_path)
    
    # ëª¨ë¸ ì •ë³´
    num_params = sum(p.numel() for p in model.parameters())
    num_layers = getattr(model.config, "num_hidden_layers", None)
    num_heads = getattr(model.config, "num_attention_heads", None)
    hidden_dim = getattr(model.config, "hidden_size", None)
    dtype = next(model.parameters()).dtype
    bytes_per_param = 2 if dtype in (torch.float16, torch.bfloat16) else 4
    model_size_mb = (num_params * bytes_per_param) / (1024 * 1024)
    
    # í…ŒìŠ¤ãƒˆ í”„ë¡¬í”„íŠ¸
    prompts = get_test_prompts()
    
    print(f"\n{'='*60}")
    print(f"ğŸ” Evaluating: {model_path}")
    print(f"{'='*60}")
    print(f"  Parameters: {num_params:,} ({num_params/1e9:.2f}B)")
    print(f"  Device: {device}")
    print(f"  Test samples: {len(prompts)}")
    
    # ì†ë„ í‰ê°€
    print("\nâ±ï¸  Speed evaluation...")
    speed_result = evaluate_speed(model, tokenizer, device, prompts, max_new_tokens)
    print(f"  Tokens/sec: {speed_result['tokens_per_sec']:.2f}")
    print(f"  Time/token: {speed_result['time_per_token_ms']:.2f} ms")
    
    # ì„±ëŠ¥ í‰ê°€ (Perplexity)
    print("\nğŸ“Š Performance evaluation (Perplexity)...")
    perplexity = evaluate_perplexity(model, tokenizer, device)
    print(f"  Perplexity: {perplexity:.2f}")
    
    # ê²°ê³¼ ìƒì„±
    result = EvalResult(
        model_name=model_path,
        num_samples=len(prompts),
        total_time_sec=speed_result["total_time_sec"],
        total_tokens=speed_result["total_tokens"],
        tokens_per_sec=speed_result["tokens_per_sec"],
        time_per_token_ms=speed_result["time_per_token_ms"],
        perplexity=perplexity
    )
    
    # ì ìˆ˜ ê³„ì‚°
    result = calculate_score(result, baseline_result)
    
    print(f"\nğŸ¯ Score:")
    print(f"  PerfNorm: {result.perf_norm:.4f}")
    print(f"  SpeedNorm: {result.speed_norm:.4f}")
    print(f"  Final Score: {result.score:.4f}")
    
    # MLflow ê¸°ë¡
    mlflow.set_experiment(experiment_name)
    
    with mlflow.start_run(run_name=run_name or Path(model_path).name):
        # íŒŒë¼ë¯¸í„°
        mlflow.log_param("model_path", model_path)
        mlflow.log_param("device", device)
        mlflow.log_param("num_params", num_params)
        mlflow.log_param("number_of_layers", num_layers)
        mlflow.log_param("number_of_heads", num_heads)
        mlflow.log_param("hidden_dim", hidden_dim)
        mlflow.log_param("total_parameters", num_params)
        mlflow.log_param("model_size_mb", model_size_mb)
        mlflow.log_param("inference_latency_ms", result.time_per_token_ms)
        mlflow.log_param("num_samples", len(prompts))
        mlflow.log_param("max_new_tokens", max_new_tokens)
        
        # ë©”íŠ¸ë¦­
        mlflow.log_metric("tokens_per_sec", result.tokens_per_sec)
        mlflow.log_metric("time_per_token_ms", result.time_per_token_ms)
        mlflow.log_metric("perplexity", result.perplexity)
        mlflow.log_metric("perf_norm", result.perf_norm)
        mlflow.log_metric("speed_norm", result.speed_norm)
        mlflow.log_metric("score", result.score)
        
        # íƒœê·¸
        if baseline_result is None:
            mlflow.set_tag("model_type", "baseline")
            mlflow.set_tag("experiment_stage", "baseline")
        else:
            mlflow.set_tag("model_type", "compressed")
            mlflow.set_tag("experiment_stage", "compression")
        mlflow.set_tag("compression_type", "layer_drop" if "drop_layers" in Path(model_path).name else "unknown")
        mlflow.set_tag("variant_name", run_name or Path(model_path).name)
    
    print(f"\nâœ… Results logged to MLflow (experiment: {experiment_name})")
    
    # í‰ê°€ ê²°ê³¼ë¥¼ ëª¨ë¸ í´ë”ì— JSONìœ¼ë¡œ ì €ì¥ (ë³´ê³ ì„œ ìë™í™”ìš©)
    if Path(model_path).is_dir():
        eval_result_path = Path(model_path) / "eval_result.json"
        with open(eval_result_path, "w") as f:
            json.dump(asdict(result), f, indent=2)
        print(f"ğŸ’¾ Eval result saved to: {eval_result_path}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del model
    if device == "cuda":
        torch.cuda.empty_cache()
    
    return result


def save_baseline(result: EvalResult, path: str = "outputs/baseline_result.json"):
    """Baseline ê²°ê³¼ ì €ì¥"""
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        json.dump(asdict(result), f, indent=2)
    print(f"ğŸ’¾ Baseline saved to: {path}")


def load_baseline(path: str = "outputs/baseline_result.json") -> EvalResult:
    """Baseline ê²°ê³¼ ë¡œë“œ"""
    with open(path, "r") as f:
        data = json.load(f)
    return EvalResult(**data)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate EXAONE model")
    parser.add_argument("--model", type=str, default="LGAI-EXAONE/EXAONE-4.0-1.2B",
                        help="Model path or HuggingFace name")
    parser.add_argument("--baseline", action="store_true",
                        help="Evaluate as baseline (no comparison)")
    parser.add_argument("--baseline-path", type=str, default="outputs/baseline_result.json",
                        help="Path to baseline result JSON")
    parser.add_argument("--run-name", type=str, default=None,
                        help="MLflow run name")
    parser.add_argument("--max-tokens", type=int, default=64,
                        help="Max new tokens to generate")
    
    args = parser.parse_args()
    
    # Baseline ë¡œë“œ or None
    baseline_result = None
    if not args.baseline and Path(args.baseline_path).exists():
        print(f"ğŸ“‚ Loading baseline from: {args.baseline_path}")
        baseline_result = load_baseline(args.baseline_path)
    
    # í‰ê°€ ì‹¤í–‰
    result = evaluate_model(
        model_path=args.model,
        run_name=args.run_name or ("baseline" if args.baseline else None),
        baseline_result=baseline_result,
        max_new_tokens=args.max_tokens
    )
    
    # Baselineì´ë©´ ì €ì¥
    if args.baseline:
        save_baseline(result, args.baseline_path)
